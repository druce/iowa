{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with XGBoost, Ray Tune, Hyperopt and BayesOpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the best hyperparameters for complex modern machine learning algorithms is time-consuming. XGBoost, LightGBM, and neural networks have so many tuning parameters and combinations that a fine-grained grid search may be infeasible.\n",
    "\n",
    "Fortunately, modern hyperparameter tuning algos, like HyperOpt and Optuna, can run many tests concurrently on a single machine or on a cluster, accelerating the tuning process, saving time and yielding better hyperparameters. This post will demonstrate speeding hyperparameter tuning using Ray Tune, HyperOpt and BayesOpt on a clusters to significantly accelerate tuning.\n",
    "\n",
    "I will use this [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) . The response we are predicting is the log-transformed SalePrice based on house features like square feet, neighborhood location, property features like pool, condition. I already did [some feature engineering and feature selection](https://github.com/druce/iowa) and my submission was top 5% when I submitted it in 2019.\n",
    "\n",
    "Outline:\n",
    "- Baseline linear regression with no hyperparameters\n",
    "- ElasticNet with L1 and L2 regularization using ElasticNetCV hyperparameter optimization\n",
    "- ElasticNet with GridSearchCV hyperparameter optimization\n",
    "- XGBoost with sequential grid search over hyperparameter subsets with early stopping \n",
    "- XGBoost with Ray, HyperOpt and BayesOpt search algorithms\n",
    "- Accelerate advanced algorithms with a Ray cluster\n",
    "\n",
    "\n",
    "| ML Algo           | Hyperparameter search algo   | CV Error (RMSE in $)  | Time     |\n",
    "|-------------------|------------------------------|-----------------------|----------|\n",
    "| Linear Regression | None                         | $18192                |   0:01s  |\n",
    "| ElasticNet        | ElasticNetCV (Grid Search)   | $18122                |   0:02s  |          \n",
    "| ElasticNet        | GridSearchCV                 | $18061                |   0:05s  |          \n",
    "| XGB               | Sequential Grid Search       | $18783                |   36:09  |\n",
    "| XGB               | HyperOpt (128 samples)       | $18808                |   21:41  |\n",
    "| XGB               | BayesOpt                     | $18506                | 1:15:04  |\n",
    "| XGB               | Optuna                       | $18618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-12 18:47:03.597412\n",
      "numpy                1.19.1\n",
      "pandas               1.1.3\n",
      "sklearn              0.23.2\n",
      "xgboost              1.2.0\n",
      "lightgbm             2.3.0\n",
      "ray                  1.0.0\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, ElasticNetCV, Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#!conda install -y -c conda-forge  xgboost \n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import lightgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest.optuna import OptunaSearch\n",
    "from ray.tune.logger import DEFAULT_LOGGERS\n",
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "# pip install hyperopt\n",
    "# pip install optuna\n",
    "\n",
    "import wandb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'hyperparameter_optimization.ipynb'\n",
    "\n",
    "print(datetime.now())\n",
    "\n",
    "print (\"%-20s %s\"% (\"numpy\", np.__version__))\n",
    "print (\"%-20s %s\"% (\"pandas\", pd.__version__))\n",
    "print (\"%-20s %s\"% (\"sklearn\", sklearn.__version__))\n",
    "print (\"%-20s %s\"% (\"xgboost\", xgboost.__version__))\n",
    "print (\"%-20s %s\"% (\"lightgbm\", lightgbm.__version__))\n",
    "print (\"%-20s %s\"% (\"ray\", ray.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "RANDOMSTATE = 42\n",
    "np.random.seed(RANDOMSTATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZZ0DJHAB'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_random_tag(length):\n",
    "    \"\"\"random tag for experiments\"\"\"\n",
    "    letters_and_digits = string.ascii_letters + string.digits\n",
    "    result_str = ''.join((random.choice(letters_and_digits) for i in range(length)))\n",
    "    return result_str.upper()\n",
    "\n",
    "get_random_tag(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleCondition_1</th>\n",
       "      <th>SaleCondition_2</th>\n",
       "      <th>SaleCondition_5</th>\n",
       "      <th>SaleType_4</th>\n",
       "      <th>BedroomAbvGr_1</th>\n",
       "      <th>BedroomAbvGr_4</th>\n",
       "      <th>BedroomAbvGr_5</th>\n",
       "      <th>HalfBath_1</th>\n",
       "      <th>TotalBath_1.0</th>\n",
       "      <th>TotalBath_2.5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>65.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>856</td>\n",
       "      <td>1710</td>\n",
       "      <td>548.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1262</td>\n",
       "      <td>1262</td>\n",
       "      <td>460.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>68.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>920</td>\n",
       "      <td>1786</td>\n",
       "      <td>608.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>961</td>\n",
       "      <td>1717</td>\n",
       "      <td>642.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>84.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>1145</td>\n",
       "      <td>2198</td>\n",
       "      <td>836.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    YearBuilt  BsmtFullBath  FullBath  KitchenAbvGr  GarageYrBlt  LotFrontage  \\\n",
       "Id                                                                              \n",
       "1           7             1         2             1            7         65.0   \n",
       "2          34             0         2             1           34         80.0   \n",
       "3           9             1         2             1            9         68.0   \n",
       "4          95             1         1             1           12         60.0   \n",
       "5          10             1         2             1           10         84.0   \n",
       "\n",
       "    MasVnrArea  1stFlrSF  GrLivArea  GarageArea  ...  SaleCondition_1  \\\n",
       "Id                                               ...                    \n",
       "1        196.0       856       1710       548.0  ...                0   \n",
       "2          0.0      1262       1262       460.0  ...                0   \n",
       "3        162.0       920       1786       608.0  ...                0   \n",
       "4          0.0       961       1717       642.0  ...                1   \n",
       "5        350.0      1145       2198       836.0  ...                0   \n",
       "\n",
       "    SaleCondition_2  SaleCondition_5  SaleType_4  BedroomAbvGr_1  \\\n",
       "Id                                                                 \n",
       "1                 0                0           1               0   \n",
       "2                 0                0           1               0   \n",
       "3                 0                0           1               0   \n",
       "4                 0                0           1               0   \n",
       "5                 0                0           1               0   \n",
       "\n",
       "    BedroomAbvGr_4  BedroomAbvGr_5  HalfBath_1  TotalBath_1.0  TotalBath_2.5  \n",
       "Id                                                                            \n",
       "1                0               0           1              0              0  \n",
       "2                0               0           0              0              1  \n",
       "3                0               0           1              0              0  \n",
       "4                0               0           0              0              0  \n",
       "5                1               0           1              0              0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.247699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.109016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.317171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.849405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.429220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SalePrice\n",
       "Id           \n",
       "1   12.247699\n",
       "2   12.109016\n",
       "3   12.317171\n",
       "4   11.849405\n",
       "5   12.429220"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import train data\n",
    "df = pd.read_pickle('df_train.pickle')\n",
    "\n",
    "response = 'SalePrice'\n",
    "predictors = ['YearBuilt',\n",
    "              'BsmtFullBath',\n",
    "              'FullBath',\n",
    "              'KitchenAbvGr',\n",
    "              'GarageYrBlt',\n",
    "              'LotFrontage',\n",
    "              'MasVnrArea',\n",
    "              '1stFlrSF',\n",
    "              'GrLivArea',\n",
    "              'GarageArea',\n",
    "              'WoodDeckSF',\n",
    "              'PorchSF',\n",
    "              'AvgBltRemod',\n",
    "              'FireBathRatio',\n",
    "              'TotalSF x OverallQual x OverallCond',\n",
    "              'AvgBltRemod x Functional x TotalFinSF',\n",
    "              'Functional x OverallQual',\n",
    "              'KitchenAbvGr x KitchenQual',\n",
    "              'GarageCars x GarageYrBlt',\n",
    "              'GarageQual x GarageCond x GarageCars',\n",
    "              'HeatingQC x Heating',\n",
    "              'monthnum',\n",
    "              'log_YearBuilt',\n",
    "              'log_LotArea',\n",
    "              'log_TotalFinSF',\n",
    "              'log_GarageRatio',\n",
    "              'log_TotalSF x OverallQual x OverallCond',\n",
    "              'log_TotalSF x OverallCond',\n",
    "              'log_AvgBltRemod x TotalFinSF',\n",
    "              'sq_2ndFlrSF',\n",
    "              'sq_BsmtFinSF',\n",
    "              'sq_BsmtFinSF x BsmtQual',\n",
    "              'sq_BsmtFinSF x BsmtBath',\n",
    "              'BldgType_4',\n",
    "              'BsmtExposure_1',\n",
    "              'BsmtExposure_4',\n",
    "              'BsmtFinType1_1',\n",
    "              'BsmtFinType1_2',\n",
    "              'BsmtFinType1_4',\n",
    "              'BsmtFinType1_5',\n",
    "              'BsmtFinType1_6',\n",
    "              'CentralAir_0',\n",
    "              'CentralAir_1',\n",
    "              'Condition1_1',\n",
    "              'Condition1_3',\n",
    "              'ExterCond_2',\n",
    "              'ExterQual_2',\n",
    "              'Exterior1st_4',\n",
    "              'Exterior1st_5',\n",
    "              'Exterior1st_10',\n",
    "              'Fence_0',\n",
    "              'Fence_2',\n",
    "              'Foundation_1',\n",
    "              'Foundation_5',\n",
    "              'GarageCars_1',\n",
    "              'GarageFinish_2',\n",
    "              'GarageFinish_3',\n",
    "              'GarageType_2',\n",
    "              'HouseStyle_2',\n",
    "              'KitchenQual_4',\n",
    "              'LotConfig_0',\n",
    "              'LotConfig_4',\n",
    "              'MSSubClass_30',\n",
    "              'MSSubClass_70',\n",
    "              'MSZoning_0',\n",
    "              'MSZoning_1',\n",
    "              'MSZoning_4',\n",
    "              'MasVnrType_2',\n",
    "              'MasVnrType_3',\n",
    "              'MoSold_1',\n",
    "              'MoSold_5',\n",
    "              'MoSold_6',\n",
    "              'MoSold_11',\n",
    "              'Neighborhood_3',\n",
    "              'Neighborhood_4',\n",
    "              'Neighborhood_5',\n",
    "              'Neighborhood_10',\n",
    "              'Neighborhood_11',\n",
    "              'Neighborhood_16',\n",
    "              'Neighborhood_17',\n",
    "              'Neighborhood_19',\n",
    "              'Neighborhood_22',\n",
    "              'Neighborhood_24',\n",
    "              'OverallCond_7',\n",
    "              'OverallQual_5',\n",
    "              'OverallQual_6',\n",
    "              'OverallQual_7',\n",
    "              'OverallQual_9',\n",
    "              'PavedDrive_0',\n",
    "              'PavedDrive_2',\n",
    "              'SaleCondition_1',\n",
    "              'SaleCondition_2',\n",
    "              'SaleCondition_5',\n",
    "              'SaleType_4',\n",
    "              'BedroomAbvGr_1',\n",
    "              'BedroomAbvGr_4',\n",
    "              'BedroomAbvGr_5',\n",
    "              'HalfBath_1',\n",
    "              'TotalBath_1.0',\n",
    "              'TotalBath_2.5']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df[response], test_size=.25)\n",
    "\n",
    "display(df[predictors].head())\n",
    "display(df[[response]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are training on a response which is the log of 1 + the sale price\n",
    "# transform prediction back to original basis with expm1 and evaluate vs. original\n",
    "\n",
    "def evaluate(y_train, y_pred_train, y_test, y_pred_test):\n",
    "    \"\"\"evaluate in train_test split\"\"\"\n",
    "    print('Train RMSE', np.sqrt(mean_squared_error(np.expm1(y_train), np.expm1(y_pred_train))))\n",
    "    print('Train R-squared', r2_score(np.expm1(y_train), np.expm1(y_pred_train)))\n",
    "    print('Train MAE', mean_absolute_error(np.expm1(y_train), np.expm1(y_pred_train)))\n",
    "    print()\n",
    "    print('Test RMSE', np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_pred_test))))\n",
    "    print('Test R-squared', r2_score(np.expm1(y_test), np.expm1(y_pred_test)))\n",
    "    print('Test MAE', mean_absolute_error(np.expm1(y_test), np.expm1(y_pred_test)))\n",
    "\n",
    "MEAN_RESPONSE=df[response].mean()\n",
    "def cv_to_raw(cv_val):\n",
    "    \"\"\"convert log1p rmse to underlying SalePrice error\"\"\"\n",
    "    return np.expm1(MEAN_RESPONSE+cv_val) - np.expm1(MEAN_RESPONSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always use same k-folds for reproducibility\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=RANDOMSTATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline linear regression\n",
    "- Raw CV RMSE 18191.9791\n",
    "- Wall time 2.81 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "100 predictors\n",
      "Train RMSE 16551.163618336377\n",
      "Train R-squared 0.955449013100165\n",
      "Train MAE 10998.86038565046\n",
      "\n",
      "Test RMSE 17846.171918075626\n",
      "Test R-squared 0.9364301769543002\n",
      "Test MAE 12755.395672857885\n",
      "\n",
      "Log1p CV RMSE 0.1037 (STD 0.0099)\n",
      "Raw CV RMSE 18191.9791 (STD 1838.6678)\n",
      "CPU times: user 125 ms, sys: 67.4 ms, total: 193 ms\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tune lr search space for alphas and l1_ratio\n",
    "print(\"LinearRegression\")\n",
    "\n",
    "print(len(predictors), \"predictors\")\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "#train and evaluate in train/test split\n",
    "lr.fit(X_train[predictors], y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train[predictors])\n",
    "y_pred_test = lr.predict(X_test[predictors])\n",
    "evaluate(y_train, y_pred_train, y_test, y_pred_test)\n",
    "\n",
    "# evaluate using kfolds, same process as train/test split but average results over 10 folds\n",
    "# more sample-efficient, less CPU-efficient\n",
    "\n",
    "scores = -cross_val_score(lr, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.04f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.04f (STD %.04f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native Sklearn xxxCV\n",
    "- LogisticRegressionCV, LassoCV, RidgeCV, ElasticNetCV, etc.\n",
    "- Test many hyperparameters in parallel with multithreading\n",
    "- Note improvement vs. LinearRegression due to controlling overfitting\n",
    "- RMSE $18103\n",
    "- Time 5s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticnetCV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "..............................................................................................................................................................................................................................................................................................................................................................................................................................[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    1.5s\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    3.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE 16782.09907754634\n",
      "Train R-squared 0.954197115772813\n",
      "Train MAE 11025.896226946616\n",
      "\n",
      "Test RMSE 17457.333905669522\n",
      "Test R-squared 0.9391701570474736\n",
      "Test MAE 12389.820674779758\n",
      "l1_ratio 0.01\n",
      "alpha 0.005623413251903491\n",
      "\n",
      "Log1p CV RMSE 0.1033 (STD 0.0112)\n",
      "Raw CV RMSE 18122.0127 (STD 2074.9545)\n",
      "CPU times: user 4.62 s, sys: 1.9 s, total: 6.53 s\n",
      "Wall time: 4.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tune elasticnet search space for alphas and L1_ratio\n",
    "# predictor selection used to create the training set used lasso\n",
    "# so l1 parameter is close to 0\n",
    "# could use ridge (eg elasticnet with 0 L1 regularization)\n",
    "# but then only 1 param, more general and useful to do this with elasticnet\n",
    "print(\"ElasticnetCV\")\n",
    "\n",
    "# make pipeline\n",
    "# with regularization must scale predictors\n",
    "elasticnetcv = make_pipeline(RobustScaler(),\n",
    "                             ElasticNetCV(max_iter=100000, \n",
    "                                          l1_ratio=[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99],\n",
    "                                          alphas=np.logspace(-4, -2, 9),\n",
    "                                          cv=kfolds,\n",
    "                                          n_jobs=-1,\n",
    "                                          verbose=1,\n",
    "                                         ))\n",
    "\n",
    "#train and evaluate in train/test split\n",
    "elasticnetcv.fit(X_train[predictors], y_train)\n",
    "\n",
    "y_pred_train = elasticnetcv.predict(X_train[predictors])\n",
    "y_pred_test = elasticnetcv.predict(X_test[predictors])\n",
    "evaluate(y_train, y_pred_train, y_test, y_pred_test)\n",
    "l1_ratio = elasticnetcv._final_estimator.l1_ratio_\n",
    "alpha = elasticnetcv._final_estimator.alpha_\n",
    "print('l1_ratio', l1_ratio)\n",
    "print('alpha', alpha)\n",
    "\n",
    "# evaluate using kfolds on full dataset\n",
    "# I don't see API to get CV error from elasticnetcv, so we use cross_val_score\n",
    "elasticnet = ElasticNet(alpha=alpha,\n",
    "                        l1_ratio=l1_ratio,\n",
    "                        max_iter=10000)\n",
    "\n",
    "scores = -cross_val_score(elasticnet, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.04f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.04f (STD %.04f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "- Useful for algos with no native multithreaded xxxCV\n",
    "- Test many hyperparameter combinations in parallel with multithreading\n",
    "- Similar result vs ElasticNetCV, not exact, need more research as to why\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV on full dataset\n",
      "Fitting 10 folds for each of 117 candidates, totalling 1170 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 392 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1170 out of 1170 | elapsed:   32.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params {'alpha': 0.0031622776601683794, 'l1_ratio': 0.01}\n",
      "best score 0.010637685614240404\n",
      "ElasticNet(alpha=0.0031622776601683794, l1_ratio=0.01, max_iter=100000)\n",
      "\n",
      "Log1p CV RMSE 0.103003 (STD 0.0109)\n",
      "Raw CV RMSE 18060.902698 (STD 2008.2407)\n",
      "weighted average 0.103023\n",
      "CPU times: user 2.26 s, sys: 112 ms, total: 2.37 s\n",
      "Wall time: 33.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gs = make_pipeline(RobustScaler(),\n",
    "                   GridSearchCV(ElasticNet(max_iter=100000),\n",
    "                                param_grid={'l1_ratio': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99],\n",
    "                                            'alpha': np.logspace(-4, -2, 9),\n",
    "                                           },\n",
    "                                scoring='neg_mean_squared_error',\n",
    "                                refit=True,\n",
    "                                cv=kfolds,\n",
    "                                n_jobs=-1,\n",
    "                                verbose=1\n",
    "                               ))\n",
    "\n",
    "# do cv using kfolds on full dataset\n",
    "print(\"\\nCV on full dataset\")\n",
    "gs.fit(df[predictors], df[response])\n",
    "print('best params', gs._final_estimator.best_params_)\n",
    "print('best score', -gs._final_estimator.best_score_)\n",
    "l1_ratio = gs._final_estimator.best_params_['l1_ratio']\n",
    "alpha = gs._final_estimator.best_params_['alpha']\n",
    "\n",
    "elasticnet = ElasticNet(alpha=alpha,\n",
    "                        l1_ratio=l1_ratio,\n",
    "                        max_iter=100000)\n",
    "print(elasticnet)\n",
    "\n",
    "scores = -cross_val_score(elasticnet, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.06f (STD %.04f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "\n",
    "# difference in average CV scores reported by GridSearchCV and cross_val_score\n",
    "# with same alpha, l1_ratio, kfolds\n",
    "# one reason could be that we used simple average, GridSearchCV is weighted by # of samples per fold?\n",
    "nsamples = [len(z[1]) for z in kfolds.split(df)]\n",
    "print(\"weighted average %.06f\" % np.average(scores, weights=nsamples))\n",
    "# not sure why, also ElasticSearchCV shows fewer fits, takes less time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roll-our-own CV \n",
    "# matches cross_val_score\n",
    "alpha = 0.0031622776601683794\n",
    "l1_ratio = 0.01\n",
    "regressor = ElasticNet(alpha=alpha,\n",
    "                       l1_ratio=l1_ratio,\n",
    "                       max_iter=10000)\n",
    "print(regressor)\n",
    "cverrors = []\n",
    "for train_fold, cv_fold in kfolds.split(df): \n",
    "    fold_X_train=df[predictors].values[train_fold]\n",
    "    fold_y_train=df[response].values[train_fold]\n",
    "    fold_X_test=df[predictors].values[cv_fold]\n",
    "    fold_y_test=df[response].values[cv_fold]\n",
    "    regressor.fit(fold_X_train, fold_y_train)\n",
    "    y_pred_test=regressor.predict(fold_X_test)\n",
    "    cverrors.append(np.sqrt(mean_squared_error(fold_y_test, y_pred_test)))\n",
    "    \n",
    "print(\"%.06f\" % np.average(cverrors))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost CV \n",
    "- XGBoost has native multithreading, CV\n",
    "- XGBoost has many tuning parameters so a complete grid search has an unreasonable number of combinations\n",
    "- We tune reduced sets sequentially and use early stopping. \n",
    "\n",
    "### Tuning methodology\n",
    "- Set an initial set of starting parameters\n",
    "- Do 10-fold CV\n",
    "- Use early stopping to halt training in each fold if no improvement after eg 100 rounds, pick hyperparameters to minimize average error over kfolds\n",
    "- Tune sequentially on groups of hyperparameters that don't interact too much between groups to reduce combinations\n",
    "- Tune max_depth and min_child_weight \n",
    "- Tune subsample and colsample_bytree\n",
    "- Tune alpha, lambda and gamma (regularization)\n",
    "- Tune learning rate: lower learning rate will need more rounds/n_estimators\n",
    "- Retrain on full dataset with best learning rate and best n_estimators (average stopping point over kfolds)\n",
    "\n",
    "### Notes\n",
    "- It doesn't seem possible to get XGBoost early stopping and also use GridSearchCV. GridSearchCV doesn't pass the kfolds in a way that XGboost understands for early stopping\n",
    "- 2 alternative approaches \n",
    "    - use native xgboost .cv which understands early stopping but doesn't use sklearn API (uses DMatrix, not np array or dataframe)\n",
    "    - use sklearn API and roll our own grid search instead of GridSearchCV (used below)\n",
    "- XGboost terminology differs from sklearn\n",
    "    - boost_rounds = n_estimators\n",
    "    - eta = learning_rate\n",
    "- parameter reference: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "- training reference: https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training\n",
    "- times are wall times on an amazon t2.2xlarge instance with \n",
    "- to set up environment:\n",
    "    - `conda create --name hyperparam python=3.8`\n",
    "    - `conda activate hyperparam`\n",
    "    - `conda install jupyter`\n",
    "    - `pip install -r requirements.txt`\n",
    "- round 1 Wall time: 6min 23s\n",
    "- round 2 Wall time: 19min 22s\n",
    "- round 3 Wall time: 5min 30s\n",
    "- round 4 Wall time: 4min 54s\n",
    "- total time 36:09\n",
    "- RMSE 18783.117031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# this cell runs a single round\n",
    "# the full process is \n",
    "# set initial XGboost parameters\n",
    "# remove overrides (search for TODO: in this cell)\n",
    "# run round 1 and override initial max_depth, min_child_weight based on best values (search for TODO:)\n",
    "# run round 2 and override subsample and colsample_bytree based on best values\n",
    "# run round 3 and override reg_alpha, reg_lambda, reg_gamma based on best values\n",
    "# run round 4 and obtain learning_rate and best n_iterations\n",
    "# this is not an exhaustive list but a representative list of most important parameters to tune\n",
    "# see https://xgboost.readthedocs.io/en/latest/parameter.html for all parameters\n",
    "\n",
    "# in XGBoost > 1.0.2 this seems to give a warning\n",
    "# Parameters: { early_stopping_rounds } might not be used.\n",
    "# but early stopping seems to be used correctly\n",
    "\n",
    "max_depth = 5\n",
    "min_child_weight=5\n",
    "colsample_bytree = 0.5\n",
    "subsample = 0.5\n",
    "reg_alpha = 1e-05\n",
    "reg_lambda = 1\n",
    "reg_gamma = 0\n",
    "learning_rate = 0.01\n",
    "\n",
    "BOOST_ROUNDS=50000   # we use early stopping so make this arbitrarily high\n",
    "EARLY_STOPPING_ROUNDS=100 # stop if no improvement after 100 rounds\n",
    "\n",
    "# round 1: tune depth and min_child_weight\n",
    "max_depths = list(range(1,5))\n",
    "min_child_weights = list(range(1,5))\n",
    "gridsearch_params_1 = product(max_depths, min_child_weights)\n",
    "\n",
    "# round 2: tune subsample and colsample_bytree\n",
    "subsamples = np.linspace(0.1, 1.0, 10)\n",
    "colsample_bytrees = np.linspace(0.1, 1.0, 10)\n",
    "gridsearch_params_2 = product(subsamples, colsample_bytrees)\n",
    "\n",
    "# round 2 (refined): tune subsample and colsample_bytree\n",
    "subsamples = np.linspace(0.4, 0.8, 9)\n",
    "colsample_bytrees = np.linspace(0.05, 0.25, 5)\n",
    "gridsearch_params_2 = product(subsamples, colsample_bytrees)\n",
    "\n",
    "# round 3: tune alpha, lambda, gamma\n",
    "reg_alphas = np.logspace(-3, -2, 3)\n",
    "reg_lambdas = np.logspace(-2, 1, 4)\n",
    "reg_gammas = [0]\n",
    "#reg_gammas = np.linspace(0, 5, 6)\n",
    "gridsearch_params_3 = product(reg_alphas, reg_lambdas, reg_gammas)\n",
    "\n",
    "# round 4: learning rate\n",
    "learning_rates = reversed(np.logspace(-3, -1, 5).tolist())\n",
    "gridsearch_params_4 = learning_rates\n",
    "\n",
    "# TODO: remove these overrides to reset the search\n",
    "# override initial parameters after search\n",
    "# round 1:\n",
    "max_depth=2\n",
    "min_child_weight=2\n",
    "# # round 2:\n",
    "subsample=0.60\n",
    "colsample_bytree=0.05\n",
    "# # round 3:  \n",
    "reg_alpha = 0.003162\n",
    "reg_lambda = 0.1\n",
    "reg_gamma = 0\n",
    "\n",
    "def my_cv(df, predictors, response, kfolds, regressor, verbose=False):\n",
    "    \"\"\"Roll our own CV over kfolds with early stopping\"\"\"\n",
    "    metrics = []\n",
    "    best_iterations = []\n",
    "\n",
    "    for train_fold, cv_fold in kfolds.split(df): \n",
    "        fold_X_train=df[predictors].values[train_fold]\n",
    "        fold_y_train=df[response].values[train_fold]\n",
    "        fold_X_test=df[predictors].values[cv_fold]\n",
    "        fold_y_test=df[response].values[cv_fold]\n",
    "        regressor.fit(fold_X_train, fold_y_train,\n",
    "                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                      eval_set=[(fold_X_test, fold_y_test)],\n",
    "                      eval_metric='rmse',\n",
    "                      verbose=verbose\n",
    "                     )\n",
    "        y_pred_test=regressor.predict(fold_X_test)\n",
    "        metrics.append(np.sqrt(mean_squared_error(fold_y_test, y_pred_test)))\n",
    "        best_iterations.append(xgb.best_iteration)\n",
    "    return np.average(metrics), np.std(metrics), np.average(best_iterations)\n",
    "\n",
    "results = []\n",
    "best_iterations = []\n",
    "\n",
    "# TODO: iteratively uncomment 1 of the following 4 lines\n",
    "# for i, (max_depth, min_child_weight) in enumerate(gridsearch_params_1): # round 1\n",
    "# for i, (subsample, colsample_bytree) in enumerate(gridsearch_params_2): # round 2\n",
    "# for i, (reg_alpha, reg_lambda, reg_gamma) in enumerate(gridsearch_params_3): # round 3\n",
    "for i, learning_rate in enumerate(gridsearch_params_4): # round 4\n",
    "\n",
    "    params = {\n",
    "        'max_depth': max_depth,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'reg_alpha': reg_alpha,\n",
    "        'reg_lambda': reg_lambda,\n",
    "        'gamma': reg_gamma,\n",
    "        'learning_rate': learning_rate,\n",
    "    }\n",
    "    print(\"%s params  %3d: %s\" % (datetime.strftime(datetime.now(), \"%T\"), i, params))\n",
    "    xgb = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=BOOST_ROUNDS,\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "        random_state=RANDOMSTATE,    \n",
    "        verbosity=1,\n",
    "        n_jobs=-1,\n",
    "        booster='gbtree',   \n",
    "        base_score=0.5, \n",
    "        scale_pos_weight=1        \n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    metric_rmse, metric_std, best_iteration = my_cv(df, predictors, response, kfolds, xgb, verbose=False)    \n",
    "    results.append([max_depth, min_child_weight, subsample, colsample_bytree, reg_alpha, reg_lambda, reg_gamma, \n",
    "                   learning_rate, metric_rmse, metric_std, best_iteration])\n",
    "    \n",
    "    print(\"%s %3d result mean: %.6f std: %.6f, iter: %.2f\" % (datetime.strftime(datetime.now(), \"%T\"), i, metric_rmse, metric_std, best_iteration))\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['max_depth', 'min_child_weight', 'subsample', 'colsample_bytree', \n",
    "                               'reg_alpha', 'reg_lambda', 'reg_gamma', 'learning_rate', 'rmse', 'std', 'best_iter']).sort_values('rmse')\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = int(results_df.iloc[0]['max_depth'])\n",
    "min_child_weight = results_df.iloc[0]['min_child_weight']\n",
    "subsample = results_df.iloc[0]['subsample']\n",
    "colsample_bytree = results_df.iloc[0]['colsample_bytree']\n",
    "reg_alpha = results_df.iloc[0]['reg_alpha']\n",
    "reg_lambda = results_df.iloc[0]['reg_lambda']\n",
    "reg_gamma = results_df.iloc[0]['reg_gamma']\n",
    "learning_rate = results_df.iloc[0]['learning_rate']\n",
    "N_ESTIMATORS = int(results_df.iloc[0]['best_iter'])\n",
    "\n",
    "params = {\n",
    "    'max_depth': int(max_depth),\n",
    "    'min_child_weight': min_child_weight,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'reg_alpha': reg_alpha,\n",
    "    'reg_lambda': reg_lambda,\n",
    "    'gamma': reg_gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators': N_ESTIMATORS,    \n",
    "}\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# evaluate without early stopping\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    booster='gbtree',   \n",
    "    base_score=0.5, \n",
    "    scale_pos_weight=1        \n",
    "    **params\n",
    ")\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds,\n",
    "                          n_jobs=-1)\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.06f (STD %.04f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor for ray.tune\n",
    "def my_xgb(config):\n",
    "    \n",
    "    # fix these configs \n",
    "    config['max_depth'] += 2   # hyperopt needs left to start at 0 but we want to start at 2\n",
    "    config['max_depth'] = int(config['max_depth'])\n",
    "    config['n_estimators'] = int(config['n_estimators'])   # pass float eg loguniform distribution, use int\n",
    "    \n",
    "    xgb = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_jobs=1,\n",
    "        random_state=RANDOMSTATE,\n",
    "        booster='gbtree',   \n",
    "        base_score=0.5, \n",
    "        scale_pos_weight=1, \n",
    "        **config,\n",
    "    )\n",
    "    scores = np.sqrt(-cross_val_score(xgb, df[predictors], df[response],\n",
    "                                      scoring=\"neg_mean_squared_error\",\n",
    "                                      cv=kfolds))\n",
    "    tune.report(mse=np.mean(scores))\n",
    "    return {'mse': np.mean(scores)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_depth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-35d48b6b8499>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m config = {\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'min_child_weight'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'subsample'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'colsample_bytree'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_depth' is not defined"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'max_depth': max_depth-2,\n",
    "    'min_child_weight': min_child_weight,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'reg_alpha': reg_alpha,\n",
    "    'reg_lambda': reg_lambda,\n",
    "    'gamma': reg_gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators': N_ESTIMATORS,\n",
    "}\n",
    "\n",
    "xgb = my_xgb(config)\n",
    "\n",
    "print(xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperOpt\n",
    "https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf\n",
    "https://github.com/hyperopt/hyperopt\n",
    "http://hyperopt.github.io/hyperopt/\n",
    "https://blog.dominodatalab.com/hyperopt-bayesian-hyperparameter-optimization/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 1.8/7.5 GiB<br>Using AsyncHyperBand: num_stopped=27\n",
       "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: -0.11425205847837049<br>Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/3.71 GiB heap, 0.0/1.27 GiB objects<br>Result logdir: /home/ubuntu/ray_results/xgb_hyperopt<br>Number of trials: 256 (211 PENDING, 1 RUNNING, 44 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name     </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  colsample_bytree</th><th style=\"text-align: right;\">  gamma</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  min_child_weight</th><th style=\"text-align: right;\">  n_estimators</th><th style=\"text-align: right;\">  reg_alpha</th><th style=\"text-align: right;\">  reg_lambda</th><th style=\"text-align: right;\">  subsample</th><th>wandb/api_key_file  </th><th>wandb/log_config  </th><th>wandb/project  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>my_xgb_7642264e</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.55</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00103425</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      1067.63 </td><td style=\"text-align: right;\">0.0064779  </td><td style=\"text-align: right;\"> 0.00106439 </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_764754de</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.2 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00504001</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       785.047</td><td style=\"text-align: right;\">0.00204532 </td><td style=\"text-align: right;\">42.2658     </td><td style=\"text-align: right;\">       0.4 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_764e6e7c</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.5 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00376611</td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      3546.25 </td><td style=\"text-align: right;\">0.116935   </td><td style=\"text-align: right;\"> 0.000130113</td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_76539334</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.65</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0221297 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">       198.946</td><td style=\"text-align: right;\">0.000361749</td><td style=\"text-align: right;\"> 0.558593   </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_7658d696</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.8 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00642388</td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">       148.264</td><td style=\"text-align: right;\">0.000141496</td><td style=\"text-align: right;\"> 0.420565   </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_765f09a8</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0600033 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">      1193.5  </td><td style=\"text-align: right;\">0.0473921  </td><td style=\"text-align: right;\"> 0.0627479  </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_7664d0e0</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.55</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0596483 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      2072.44 </td><td style=\"text-align: right;\">0.145667   </td><td style=\"text-align: right;\"> 1.02135    </td><td style=\"text-align: right;\">       0.55</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_7669f46c</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.05</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00986594</td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       668.603</td><td style=\"text-align: right;\">0.0136262  </td><td style=\"text-align: right;\"> 6.39466    </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_766fc932</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.3 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.014299  </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      5551.71 </td><td style=\"text-align: right;\">0.484234   </td><td style=\"text-align: right;\"> 1.99934    </td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_76765612</td><td>PENDING   </td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0280841 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">       296.721</td><td style=\"text-align: right;\">0.00027662 </td><td style=\"text-align: right;\">22.5591     </td><td style=\"text-align: right;\">       0.45</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_761c0dd8</td><td>RUNNING   </td><td>     </td><td style=\"text-align: right;\">              0.7 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0923905 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      8696.81 </td><td style=\"text-align: right;\">0.0159487  </td><td style=\"text-align: right;\"> 0.10013    </td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>my_xgb_752cc2d2</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.7 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0105391 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">       435.406</td><td style=\"text-align: right;\">0.000188888</td><td style=\"text-align: right;\"> 9.90358    </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        18.6349 </td><td style=\"text-align: right;\">0.203631</td></tr>\n",
       "<tr><td>my_xgb_7535733c</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.3 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00673344</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       428.153</td><td style=\"text-align: right;\">0.538855   </td><td style=\"text-align: right;\"> 2.32463    </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.4062 </td><td style=\"text-align: right;\">0.681993</td></tr>\n",
       "<tr><td>my_xgb_753995c0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.15</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00133237</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">      7988.18 </td><td style=\"text-align: right;\">0.00163546 </td><td style=\"text-align: right;\">19.9476     </td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       314.914  </td><td style=\"text-align: right;\">0.114781</td></tr>\n",
       "<tr><td>my_xgb_753f0096</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.2 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0187659 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">       735.64 </td><td style=\"text-align: right;\">0.000262686</td><td style=\"text-align: right;\">32.5338     </td><td style=\"text-align: right;\">       0.6 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        26.6781 </td><td style=\"text-align: right;\">0.11581 </td></tr>\n",
       "<tr><td>my_xgb_754418ba</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.6 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00155207</td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      4032.08 </td><td style=\"text-align: right;\">0.0246711  </td><td style=\"text-align: right;\"> 0.00107456 </td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       185.774  </td><td style=\"text-align: right;\">0.124603</td></tr>\n",
       "<tr><td>my_xgb_754995f6</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.15</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00106609</td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">      2700.39 </td><td style=\"text-align: right;\">0.0127785  </td><td style=\"text-align: right;\"> 0.0424791  </td><td style=\"text-align: right;\">       0.8 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        72.5166 </td><td style=\"text-align: right;\">0.661069</td></tr>\n",
       "<tr><td>my_xgb_754e8a70</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.4 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00556109</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       238.15 </td><td style=\"text-align: right;\">0.000451262</td><td style=\"text-align: right;\">43.2731     </td><td style=\"text-align: right;\">       0.75</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.67665</td><td style=\"text-align: right;\">3.25644 </td></tr>\n",
       "<tr><td>my_xgb_7553a546</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.55</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.0154611 </td><td style=\"text-align: right;\">          0</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">      5605.97 </td><td style=\"text-align: right;\">0.00272955 </td><td style=\"text-align: right;\"> 4.48938    </td><td style=\"text-align: right;\">       0.7 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       266.984  </td><td style=\"text-align: right;\">0.109036</td></tr>\n",
       "<tr><td>my_xgb_75592052</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.5 </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00126552</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">      4734.92 </td><td style=\"text-align: right;\">0.00967981 </td><td style=\"text-align: right;\">33.1173     </td><td style=\"text-align: right;\">       0.65</td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       356.093  </td><td style=\"text-align: right;\">0.1519  </td></tr>\n",
       "<tr><td>my_xgb_755e3dee</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">              0.75</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     0.00153686</td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">       678.644</td><td style=\"text-align: right;\">0.306625   </td><td style=\"text-align: right;\"> 1.59856    </td><td style=\"text-align: right;\">       0.9 </td><td>secrets/wandb.txt   </td><td>True              </td><td>iowa           </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        28.1163 </td><td style=\"text-align: right;\">4.08285 </td></tr>\n",
       "</tbody>\n",
       "</table><br>... 236 more trials not shown (201 PENDING, 34 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m [19:26:29] WARNING: ../src/learner.cc:516: \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m Parameters: { wandb } might not be used.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   This may not be accurate due to some parameters are only used in language bindings but\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   verification. Please open an issue if you find above cases.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m [19:26:44] WARNING: ../src/learner.cc:516: \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m Parameters: { wandb } might not be used.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   This may not be accurate due to some parameters are only used in language bindings but\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   verification. Please open an issue if you find above cases.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m [19:26:44] WARNING: ../src/learner.cc:516: \n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m Parameters: { wandb } might not be used.\n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m   This may not be accurate due to some parameters are only used in language bindings but\n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m   passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m   verification. Please open an issue if you find above cases.\n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m [19:26:59] WARNING: ../src/learner.cc:516: \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m Parameters: { wandb } might not be used.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   This may not be accurate due to some parameters are only used in language bindings but\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   verification. Please open an issue if you find above cases.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m [19:27:13] WARNING: ../src/learner.cc:516: \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m Parameters: { wandb } might not be used.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   This may not be accurate due to some parameters are only used in language bindings but\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   verification. Please open an issue if you find above cases.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m [19:27:29] WARNING: ../src/learner.cc:516: \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m Parameters: { wandb } might not be used.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   This may not be accurate due to some parameters are only used in language bindings but\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   verification. Please open an issue if you find above cases.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m [19:27:43] WARNING: ../src/learner.cc:516: \n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m Parameters: { wandb } might not be used.\n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m   This may not be accurate due to some parameters are only used in language bindings but\n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m   passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m   verification. Please open an issue if you find above cases.\n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7642)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m [19:27:44] WARNING: ../src/learner.cc:516: \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m Parameters: { wandb } might not be used.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   This may not be accurate due to some parameters are only used in language bindings but\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   verification. Please open an issue if you find above cases.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m [19:28:00] WARNING: ../src/learner.cc:516: \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m Parameters: { wandb } might not be used.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   This may not be accurate due to some parameters are only used in language bindings but\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   verification. Please open an issue if you find above cases.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m [19:28:15] WARNING: ../src/learner.cc:516: \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m Parameters: { wandb } might not be used.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   This may not be accurate due to some parameters are only used in language bindings but\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m   verification. Please open an issue if you find above cases.\n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8389)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES=256\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = HyperOptSearch(random_state_seed=RANDOMSTATE)\n",
    "# uncomment and set max_concurrent to limit number of cores\n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"config\": {\n",
    "        \"n_estimators\": tune.loguniform(100, 10000),\n",
    "        \"max_depth\": tune.randint(0, 6),\n",
    "        'min_child_weight': tune.randint(0, 6),\n",
    "        \"subsample\": tune.quniform(0.4, 0.9, 0.05),\n",
    "        \"colsample_bytree\": tune.quniform(0.05, 0.8, 0.05),\n",
    "        \"reg_alpha\": tune.loguniform(1e-04, 1),\n",
    "        \"reg_lambda\": tune.loguniform(1e-04, 100),\n",
    "        \"gamma\": 0,\n",
    "        \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "        \"wandb\": {\n",
    "            \"project\": \"iowa\",\n",
    "            \"api_key_file\": \"secrets/wandb.txt\",\n",
    "            \"log_config\": True\n",
    "        }    \n",
    "    }\n",
    "}\n",
    "\n",
    "analysis = tune.run(my_xgb,\n",
    "                    name=\"xgb_hyperopt\",\n",
    "                    metric=\"mse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                    loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                    **tune_kwargs)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results_df = analysis.results_df[['mse', 'date', 'time_this_iter_s',\n",
    "       'config.n_estimators', 'config.max_depth', 'config.min_child_weight', 'config.subsample',\n",
    "       'config.colsample_bytree', 'config.reg_alpha', 'config.reg_lambda', 'config.gamma',\n",
    "       'config.learning_rate']].sort_values('mse')\n",
    "analysis_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = analysis_results_df.iloc[0]['config.max_depth']\n",
    "min_child_weight = analysis_results_df.iloc[0]['config.min_child_weight']\n",
    "subsample = analysis_results_df.iloc[0]['config.subsample']\n",
    "colsample_bytree = analysis_results_df.iloc[0]['config.colsample_bytree']\n",
    "reg_alpha = analysis_results_df.iloc[0]['config.reg_alpha']\n",
    "reg_lambda = analysis_results_df.iloc[0]['config.reg_lambda']\n",
    "reg_gamma = analysis_results_df.iloc[0]['config.gamma']\n",
    "learning_rate = analysis_results_df.iloc[0]['config.learning_rate']\n",
    "N_ESTIMATORS = analysis_results_df.iloc[0]['config.n_estimators']    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = {\n",
    "    'max_depth': max_depth,\n",
    "    'min_child_weight': min_child_weight,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'reg_alpha': reg_alpha,\n",
    "    'reg_lambda': reg_lambda,\n",
    "    'gamma': reg_gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators':  N_ESTIMATORS\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    **best_config\n",
    ")\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.06f (STD %.04f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bayesopt\n",
    "NUM_SAMPLES=256\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = BayesOptSearch(utility_kwargs={\n",
    "    \"kind\": \"ucb\",\n",
    "    \"kappa\": 2.5,\n",
    "    \"xi\": 0.0\n",
    "})\n",
    "\n",
    "# uncomment and set max_concurrent to limit number of cores\n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"config\": {\n",
    "        \"n_estimators\": tune.loguniform(100, 10000),\n",
    "        \"max_depth\": tune.quniform(0, 6, 1),\n",
    "        'min_child_weight': tune.quniform(0, 6, 1),\n",
    "        \"subsample\": tune.quniform(0.4, 0.9, 0.05),\n",
    "        \"colsample_bytree\": tune.quniform(0.05, 0.8, 0.05),\n",
    "        \"reg_alpha\": tune.loguniform(1e-04, 1),\n",
    "        \"reg_lambda\": tune.loguniform(1e-04, 100),\n",
    "        \"gamma\": 0,\n",
    "        \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "        \"wandb\": {\n",
    "            \"project\": \"iowa\",\n",
    "            \"api_key_file\": \"secrets/wandb.txt\",\n",
    "            \"log_config\": True\n",
    "        }    \n",
    "    }\n",
    "}\n",
    "\n",
    "analysis = tune.run(my_xgb,\n",
    "                    name=\"xgb_bayesopt\",\n",
    "                    metric=\"mse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                    loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                    **tune_kwargs)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results_df = analysis.results_df[['mse', 'date', 'time_this_iter_s',\n",
    "       'config.n_estimators', 'config.max_depth', 'config.min_child_weight', 'config.subsample',\n",
    "       'config.colsample_bytree', 'config.reg_alpha', 'config.reg_lambda', 'config.gamma',\n",
    "       'config.learning_rate']].sort_values('mse')\n",
    "analysis_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = analysis_results_df.iloc[0]['config.max_depth']\n",
    "min_child_weight = analysis_results_df.iloc[0]['config.min_child_weight']\n",
    "subsample = analysis_results_df.iloc[0]['config.subsample']\n",
    "colsample_bytree = analysis_results_df.iloc[0]['config.colsample_bytree']\n",
    "reg_alpha = analysis_results_df.iloc[0]['config.reg_alpha']\n",
    "reg_lambda = analysis_results_df.iloc[0]['config.reg_lambda']\n",
    "reg_gamma = analysis_results_df.iloc[0]['config.gamma']\n",
    "learning_rate = analysis_results_df.iloc[0]['config.learning_rate']\n",
    "N_ESTIMATORS = analysis_results_df.iloc[0]['config.n_estimators']    \n",
    "\n",
    "best_config = {\n",
    "    'max_depth': max_depth,\n",
    "    'min_child_weight': min_child_weight,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'reg_alpha': reg_alpha,\n",
    "    'reg_lambda': reg_lambda,\n",
    "    'gamma': reg_gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators':  N_ESTIMATORS\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    **best_config\n",
    ")\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.06f (STD %.04f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna\n",
    "NUM_SAMPLES=256\n",
    "optuna_xgb = my_xgb\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "algo = OptunaSearch()\n",
    "# uncomment and set max_concurrent to limit number of cores\n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"config\": {\n",
    "        \"n_estimators\": tune.loguniform(100, 10000),\n",
    "        \"max_depth\": tune.quniform(0, 6, 1),\n",
    "        'min_child_weight': tune.quniform(0, 6, 1),\n",
    "        \"subsample\": tune.quniform(0.4, 0.9, 0.05),\n",
    "        \"colsample_bytree\": tune.quniform(0.05, 0.8, 0.05),\n",
    "        \"reg_alpha\": tune.loguniform(1e-04, 1),\n",
    "        \"reg_lambda\": tune.loguniform(1e-04, 100),\n",
    "        \"gamma\": 0,\n",
    "        \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "        \"wandb\": {\n",
    "            \"project\": \"iowa\",\n",
    "            \"api_key_file\": \"secrets/wandb.txt\",\n",
    "            \"log_config\": True,\n",
    "            \"name\": get_random_tag(6)\n",
    "        }           \n",
    "    }\n",
    "}\n",
    "\n",
    "analysis = tune.run(optuna_xgb,\n",
    "                    name=\"xgb_optuna\",\n",
    "                    metric=\"mse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                    loggers=DEFAULT_LOGGERS + (WandbLogger, ),                    \n",
    "                    **tune_kwargs)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results_df = analysis.results_df[['mse', 'date', 'time_this_iter_s',\n",
    "       'config.n_estimators', 'config.max_depth', 'config.min_child_weight', 'config.subsample',\n",
    "       'config.colsample_bytree', 'config.reg_alpha', 'config.reg_lambda', 'config.gamma',\n",
    "       'config.learning_rate']].sort_values('mse')\n",
    "analysis_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = analysis_results_df.iloc[0]['config.max_depth']\n",
    "min_child_weight = analysis_results_df.iloc[0]['config.min_child_weight']\n",
    "subsample = analysis_results_df.iloc[0]['config.subsample']\n",
    "colsample_bytree = analysis_results_df.iloc[0]['config.colsample_bytree']\n",
    "reg_alpha = analysis_results_df.iloc[0]['config.reg_alpha']\n",
    "reg_lambda = analysis_results_df.iloc[0]['config.reg_lambda']\n",
    "reg_gamma = analysis_results_df.iloc[0]['config.gamma']\n",
    "learning_rate = analysis_results_df.iloc[0]['config.learning_rate']\n",
    "N_ESTIMATORS = analysis_results_df.iloc[0]['config.n_estimators']    \n",
    "\n",
    "best_config = {\n",
    "    'max_depth': max_depth,\n",
    "    'min_child_weight': min_child_weight,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'reg_alpha': reg_alpha,\n",
    "    'reg_lambda': reg_lambda,\n",
    "    'gamma': reg_gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators':  N_ESTIMATORS\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=RANDOMSTATE,    \n",
    "    verbosity=1,\n",
    "    n_jobs=-1,\n",
    "    **best_config\n",
    ")\n",
    "print(xgb)\n",
    "\n",
    "scores = -cross_val_score(xgb, df[predictors], df[response],\n",
    "                          scoring=\"neg_root_mean_squared_error\",\n",
    "                          cv=kfolds)\n",
    "\n",
    "raw_scores = [cv_to_raw(x) for x in scores]\n",
    "print()\n",
    "print(\"Log1p CV RMSE %.06f (STD %.04f)\" % (np.mean(scores), np.std(scores)))\n",
    "print(\"Raw CV RMSE %.06f (STD %.04f)\" % (np.mean(raw_scores), np.std(raw_scores)))\n",
    "raw_scores = [cv_to_raw(x) for x in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune LightGBM\n",
    "print(\"LightGBM\")\n",
    "#!conda install -y -c conda-forge lightgbm\n",
    "\n",
    "NUM_SAMPLES=256\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "\n",
    "def my_lgbm(config):\n",
    "    \n",
    "    # fix these configs \n",
    "    config['n_estimators'] = int(config['n_estimators'])   # pass float eg loguniform distribution, use int\n",
    "    config['num_leaves'] = 2 + int(config['num_leaves'])\n",
    "    \n",
    "    lgbm = LGBMRegressor(objective='regression',\n",
    "                         num_leaves=config['num_leaves'],\n",
    "                         learning_rate=config['learning_rate'],\n",
    "                         n_estimators=config['n_estimators'],\n",
    "                         max_bin=200,\n",
    "                         bagging_fraction=config['bagging_fraction'],\n",
    "                         feature_fraction=config['feature_fraction'],\n",
    "                         feature_fraction_seed=7,\n",
    "                         min_data_in_leaf=2,\n",
    "                         verbose=-1,\n",
    "                         # early stopping params, maybe in fit\n",
    "                         #early_stopping_rounds=early_stopping_rounds,\n",
    "                         #valid_sets=[xgtrain, xgvalid], valid_names=['train','valid'], evals_result=evals_results\n",
    "                         #num_boost_round=num_boost_round,\n",
    "                         )\n",
    "    \n",
    "    scores = np.sqrt(-cross_val_score(lgbm, df[predictors], df[response],\n",
    "                                      scoring=\"neg_mean_squared_error\",\n",
    "                                      cv=kfolds))\n",
    "    tune.report(mse=np.mean(scores))\n",
    "    return {'mse': np.mean(scores)}\n",
    "\n",
    "algo = HyperOptSearch(random_state_seed=RANDOMSTATE)\n",
    "# uncomment and set max_concurrent to limit number of cores\n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "tune_kwargs = {\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"config\": {\n",
    "        \"n_estimators\": tune.loguniform(100, 10000),\n",
    "        'num_leaves': tune.randint(0, 10),\n",
    "        \"bagging_fraction\": tune.uniform(0.5, 0.8),\n",
    "        \"feature_fraction\": tune.uniform(0.01, 0.8),\n",
    "        \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
    "        \"wandb\": {\n",
    "            \"project\": \"iowa\",\n",
    "            \"api_key_file\": \"secrets/wandb.txt\",\n",
    "            \"log_config\": True\n",
    "        }    \n",
    "    }\n",
    "}\n",
    "\n",
    "analysis = tune.run(my_lgbm,\n",
    "                    name=\"lgbm_hyperopt\",\n",
    "                    metric=\"mse\",\n",
    "                    mode=\"min\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    verbose=1,\n",
    "                    loggers=DEFAULT_LOGGERS + (WandbLogger, ),\n",
    "                    **tune_kwargs)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"%-20s %s\" % (\"Start Time\", start_time))\n",
    "print(\"%-20s %s\" % (\"End Time\", end_time))\n",
    "print(str(timedelta(seconds=(end_time-start_time).seconds)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
